{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "869a8ca5",
      "metadata": {},
      "source": [
        "# Question Answering with Retrieval Augmented Generation (RAG)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **Question Answering (QA)** using the complete RAG pipeline. Building on `1-DocumentLoading.ipynb`, where we loaded the Digital Personal Data Protection Act (DPDPA) PDF, we now focus on the **retrieval and generation stages**:\n",
        "\n",
        "```\n",
        "Load â†’ Split â†’ Embed â†’ Store â†’ Retrieve â†’ Generate\n",
        "```\n",
        "\n",
        "## Why RAG-based QA Matters\n",
        "\n",
        "1. **Grounded Responses**: LLMs answer using specific documents, reducing hallucinations\n",
        "2. **Source Attribution**: Metadata from document loading enables citation and audit trails\n",
        "3. **Domain Specialization**: Transform generic LLMs into domain experts using your documents\n",
        "4. **Up-to-date Information**: Refresh knowledge base without retraining the model\n",
        "\n",
        "## What We'll Cover\n",
        "\n",
        "1. **Vector Store Connection** - Loading persisted embeddings from document chunks\n",
        "2. **Basic Retrieval QA** - Simple question answering with default settings\n",
        "3. **Custom Prompts** - Controlling response format and preventing hallucinations\n",
        "4. **Source Document Inspection** - Tracing answers back to original PDF pages\n",
        "5. **Chain Type Comparison** - Evaluating `stuff`, `map_reduce`, and `refine` strategies\n",
        "6. **Production Best Practices** - Guidelines for deploying RAG systems\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "User Question â†’ Embed Query â†’ Similarity Search â†’ Retrieve Documents (with metadata) â†’ \n",
        "LLM + Retrieved Context â†’ Generated Answer + Citations\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c8dd98",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Environment Setup\n",
        "\n",
        "Initialize OpenAI client and load environment variables. These credentials are needed for:\n",
        "- OpenAI embeddings (to match the embedding model used when building the vector store)\n",
        "- ChatGPT models (for generating answers based on retrieved context)\n",
        "- Token counting with tiktoken (for monitoring context window usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "98cbb5f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken  # OpenAI's tokenizer for accurate token counting\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# Load environment variables from .env file (contains OPENAI_API_KEY)\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472f01bd",
      "metadata": {},
      "source": [
        "### Select LLM Model\n",
        "\n",
        "Choose the appropriate GPT-3.5 model based on availability date. This ensures compatibility with the latest model versions while maintaining backward compatibility for older notebook executions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2c31ba99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-3.5-turbo\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "current_date = datetime.datetime.now().date()\n",
        "if current_date < datetime.date(2023, 9, 2):\n",
        "    llm_name = \"gpt-3.5-turbo-0301\"\n",
        "else:\n",
        "    llm_name = \"gpt-3.5-turbo\"\n",
        "print(llm_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d94666",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Vector Store Connection\n",
        "\n",
        "### Chroma Vector Database\n",
        "\n",
        "**Chroma** is a lightweight, open-source vector database that stores:\n",
        "- **Document chunks**: Text segments from split documents\n",
        "- **Embeddings**: Vector representations (1536 dimensions for OpenAI)\n",
        "- **Metadata**: Preserved from document loading (source, page, timestamps)\n",
        "\n",
        "### The Journey from Document Loading to Vector Store\n",
        "\n",
        "In `1-DocumentLoading.ipynb`, we used `PyPDFLoader` to extract the DPDPA PDF:\n",
        "```python\n",
        "loader = PyPDFLoader(\"./99-DPDPA.pdf\")\n",
        "pages = loader.load()  # 21 pages with metadata\n",
        "```\n",
        "\n",
        "Each page became a `Document` with rich metadata:\n",
        "- `source`: `'./99-DPDPA.pdf'`\n",
        "- `page`: Page index (0-20)\n",
        "- `page_label`: Human-readable page number\n",
        "- `total_pages`: 21\n",
        "- `creator`, `producer`, `creationdate`, `moddate`: PDF properties\n",
        "\n",
        "In subsequent notebooks, these pages were:\n",
        "1. **Split** into smaller chunks for better retrieval precision\n",
        "2. **Embedded** using OpenAI's text-embedding model\n",
        "3. **Stored** in Chroma with metadata intact\n",
        "\n",
        "### Loading the Persisted Collection\n",
        "\n",
        "The next cell reconnects to the existing vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4523e5a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma.vectorstores import Chroma\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Point to the directory where embeddings were persisted\n",
        "persist_directory = 'docs/chroma/'\n",
        "\n",
        "# Recreate the SAME embedding model used when building the store\n",
        "# Critical: Must match the original embedding model for accurate retrieval\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# Load the existing vector store from disk (no re-embedding required)\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory, \n",
        "    embedding_function=embedding\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482fe572",
      "metadata": {},
      "source": [
        "### Verify Collection Size\n",
        "\n",
        "Check how many document chunks are stored in the vector database. The 21-page PDF was split into multiple chunks for more granular retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c92fb5c",
      "metadata": {},
      "source": [
        "## Quick Retrieval Sanity Check\n",
        "\n",
        "Before wiring an LLM into the loop, it is useful to verify that the vector store can **retrieve relevant pages** for a plain-language question.\n",
        "\n",
        "In the next cell we:\n",
        "- Define a natural-language `question` about penalties for data leaks under the DPDPA.\n",
        "- Call `vectordb.similarity_search(question, k=3)` to fetch the **top 3 most similar chunks**.\n",
        "- Inspect the length of `docs` to confirm we got results back.\n",
        "\n",
        "Internally, each retrieved `Document` still carries the metadata that originated during document loading (e.g., which page of `99-DPDPA.pdf` it came from)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4519ad2",
      "metadata": {},
      "source": [
        "**Result**: 55 chunks stored in the collection.\n",
        "\n",
        "The 21-page PDF was split into 55 smaller chunks (roughly 2-3 chunks per page on average). This chunking strategy balances:\n",
        "- **Granularity**: Smaller chunks enable more precise retrieval\n",
        "- **Context**: Each chunk retains enough text for coherent meaning\n",
        "- **Metadata inheritance**: All chunks preserve the original page-level metadata from document loading\n",
        "\n",
        "---\n",
        "\n",
        "## Basic Retrieval Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8fe46965",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "65363231",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What are the penalties for a data leak?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cba266f",
      "metadata": {},
      "source": [
        "**Result**: Successfully retrieved 3 chunks.\n",
        "\n",
        "The vector store found 3 document chunks related to penalties for data leaks. Each of these chunks still contains the full metadata from the original PDF loading (source file, page number, etc.), which will be crucial for providing citations when we generate answers.\n",
        "\n",
        "### How Retrieval Works Behind the Scenes\n",
        "\n",
        "```\n",
        "Question: \"What are the penalties for a data leak?\"\n",
        "         â†“\n",
        "Query Embedding (1536-dim vector via OpenAI)\n",
        "         â†“\n",
        "Cosine Similarity with all 55 stored embeddings\n",
        "         â†“\n",
        "Top 3 most similar chunks returned\n",
        "         â†“\n",
        "Each chunk includes: page_content + metadata (source, page, etc.)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Retrieval QA with LLM\n",
        "\n",
        "Now we combine **retrieval** (finding relevant chunks) with **generation** (LLM-based answer synthesis).\n",
        "\n",
        "### Initialize the Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a19e4d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize ChatGPT with temperature=0 for deterministic, factual responses\n",
        "# Lower temperature reduces creativity/hallucination, which is ideal for QA\n",
        "llm = ChatOpenAI(model=llm_name, temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4d28a7",
      "metadata": {},
      "source": [
        "### Build the RetrievalQA Chain\n",
        "\n",
        "`RetrievalQA` orchestrates the complete RAG pipeline:\n",
        "1. Takes a user query\n",
        "2. Retrieves relevant chunks from the vector store\n",
        "3. Injects retrieved chunks into the LLM prompt as context\n",
        "4. Returns the LLM's generated answer\n",
        "\n",
        "**Default Behavior**: Uses the \"stuff\" strategy (concatenates all retrieved chunks into one prompt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "95680294",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_classic.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "97e27858",
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21505f7",
      "metadata": {},
      "source": [
        "### Create the QA Chain\n",
        "\n",
        "The next cell builds a `RetrievalQA` chain by connecting:\n",
        "- The ChatGPT language model\n",
        "- The vector store retriever (which will fetch relevant chunks with their metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81f27ee",
      "metadata": {},
      "source": [
        "## Custom QA Prompt with Retrieved Context\n",
        "\n",
        "The basic `RetrievalQA` chain simply feeds retrieved `Document` text into the model with a default prompt. To make answers more **controlled and user-friendly**, we can define a custom prompt template.\n",
        "\n",
        "The next cell builds a `PromptTemplate` that:\n",
        "- Instructs the model to answer **only** using the provided context from the vector store (derived from `99-DPDPA.pdf`).\n",
        "- Limits the answer length and enforces a fixed closing phrase (\"thanks for asking!\").\n",
        "- Avoids hallucinations by explicitly telling the model to say \"I don't know\" when the context is insufficient.\n",
        "\n",
        "This prompt design complements the **high-quality document loading and metadata** from the earlier notebook, helping the LLM stay grounded in the source PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6cec17be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/l8/yvtt6g7s4mg361124qs02mx40000gn/T/ipykernel_95393/4094420968.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
            "  result = qa_chain({\"query\": question})\n"
          ]
        }
      ],
      "source": [
        "result = qa_chain({\"query\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68cd60fd",
      "metadata": {},
      "source": [
        "### Run the QA Chain\n",
        "\n",
        "Now we can ask the same question we tested earlier with retrieval-only, but this time the LLM will synthesize a natural language answer from the retrieved chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b3cbf710",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The penalties for a data leak can extend up to two hundred and fifty crore rupees, as specified in the Schedule of the Data Protection Act.'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fdab1373",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "470ceac8",
      "metadata": {},
      "source": [
        "**Result**: \"The penalties for a data leak can extend up to two hundred and fifty crore rupees, as specified in the Schedule of the Data Protection Act.\"\n",
        "\n",
        "### Analysis\n",
        "\n",
        "The LLM successfully:\n",
        "1. **Retrieved** relevant chunks from the DPDPA (loaded via `PyPDFLoader` in earlier notebooks)\n",
        "2. **Extracted** the specific penalty amount (250 crore rupees)\n",
        "3. **Cited** the source (\"Schedule of the Data Protection Act\")\n",
        "4. **Generated** a concise, factual answer\n",
        "\n",
        "This demonstrates the complete RAG pipeline working end-to-end, from document loading to answer generation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "87e31945",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6a0d4219",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"What are all the leaks in which a penalty might be incurred to a Data Fiduciary?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5897ba3",
      "metadata": {},
      "source": [
        "### Create QA Chain with Custom Prompt\n",
        "\n",
        "The next cell creates a new `RetrievalQA` chain with two key enhancements:\n",
        "- **return_source_documents=True**: Returns the retrieved chunks with their metadata (for citation and debugging)\n",
        "- **chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}**: Injects our custom prompt template\n",
        "\n",
        "This allows us to inspect which pages from the DPDPA PDF (loaded in `1-DocumentLoading.ipynb`) were used to answer each question."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e81eee5",
      "metadata": {},
      "source": [
        "## Comparing RetrievalQA Chain Types\n",
        "\n",
        "`RetrievalQA.from_chain_type` supports multiple strategies for combining information from retrieved documents:\n",
        "\n",
        "- **`stuff` (default)**: Concatenates all retrieved chunks into a single prompt (simple and fast).\n",
        "- **`map_reduce`**: Runs the model separately on each chunk (map), then merges the partial answers (reduce). Helpful when documents are long or numerous.\n",
        "- **`refine`**: Iteratively updates an answer as it sees more chunks, which can surface additional details.\n",
        "\n",
        "In the following cells we reuse the **same question and vector store**, but switch the `chain_type` to `map_reduce` and `refine` to see how different strategies interpret the same DPDPA pages that were originally loaded via `PyPDFLoader`. This illustrates how downstream QA behavior depends on both the **retrieval setup** and the **document loading quality** established earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031d07bf",
      "metadata": {},
      "source": [
        "### Test with a More Complex Question\n",
        "\n",
        "Now let's ask a more nuanced question that requires synthesizing information from multiple sections of the DPDPA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6466d939",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = qa_chain({\"query\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3c5ea0d3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A Data Fiduciary might incur a penalty for not protecting personal data, not reporting a personal data breach, not erasing personal data upon withdrawal of consent, not obtaining verifiable consent for processing personal data of a child or person with a disability, and not responding to grievances in a timely manner. Thanks for asking!'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb1d932",
      "metadata": {},
      "source": [
        "**Observation**: The custom prompt is working! Notice the answer:\n",
        "1. Uses concise language (3 sentences max as instructed)\n",
        "2. Lists multiple penalty scenarios comprehensively\n",
        "3. Ends with \"Thanks for asking!\" as specified in the template\n",
        "4. Stays grounded in the DPDPA context\n",
        "\n",
        "---\n",
        "\n",
        "## Inspecting Source Documents\n",
        "\n",
        "One of RAG's key benefits is **provenance tracking**. Because we preserved metadata during document loading (`PyPDFLoader` in `1-DocumentLoading.ipynb`), we can trace which PDF pages supported each answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a944d699",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(id='d6049676-634d-41cd-965a-6a2da68e824d', metadata={'moddate': '2023-08-12T02:14:35+05:30', 'creator': 'PyPDF', 'page': 6, 'creationdate': '2023-08-12T02:13:03+05:30', 'source': './99-DPDPA.pdf', 'producer': 'iTextSharpâ„¢ 5.5.13.1 Â©2000-2019 iText Group NV (AGPL-version)', 'page_label': '7', 'total_pages': 21}, page_content='to ensure effective observance of  the provisions of this Act and the rules made thereunder.\\n(5) A Data Fiduciary shall protect personal data in its possession or under its control,\\nincluding in respect of any processing undertaken by it or on its behalf by a Data Processor,\\nby taking reasonable security safeguards to prevent personal data breach.\\n(6) In the event of a personal data breach, the Data Fiduciary shall give the Board and\\neach affected Data Principal, intimation of such breach in such form and manner as may be\\nprescribed.\\n(7) A Data Fiduciary shall, unless retention is necessary for compliance with any law\\nfor the time being in force,â€”\\n(a) erase personal data, upon the Data Principal withdrawing her consent or as\\nGeneral\\nobligations of\\nData\\nFiduciary.\\n53 of 2005.')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"source_documents\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64c05b0a",
      "metadata": {},
      "source": [
        "### Source Document Metadata Analysis\n",
        "\n",
        "The retrieved document shows all the metadata preserved from `PyPDFLoader`:\n",
        "\n",
        "**Metadata Fields**:\n",
        "- `source`: `'./99-DPDPA.pdf'` - Original file from document loading\n",
        "- `page`: `6` - Zero-indexed page number (7th page of PDF)\n",
        "- `page_label`: `'7'` - Human-readable page number\n",
        "- `total_pages`: `21` - Full document length\n",
        "- `creator`, `producer`: PDF generation tools\n",
        "- `creationdate`, `moddate`: Document timestamps\n",
        "\n",
        "**Page Content**: Excerpt from Section 5-7, describing Data Fiduciary obligations including:\n",
        "- Protecting personal data with reasonable security safeguards\n",
        "- Reporting personal data breaches to the Board\n",
        "- Erasing personal data upon consent withdrawal\n",
        "\n",
        "This metadata enables **production features**:\n",
        "1. **Citation**: \"Answer based on DPDPA page 7\"\n",
        "2. **Audit Trail**: Track which sources influenced responses\n",
        "3. **Filtering**: Retrieve only from specific documents or date ranges\n",
        "4. **Version Control**: Handle multiple versions of the same document\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a1f3b28d",
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"map_reduce\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7dccf892",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = qa_chain_mr({\"query\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe1fecc",
      "metadata": {},
      "source": [
        "### Map-Reduce Chain Type\n",
        "\n",
        "The map-reduce approach processes each retrieved chunk independently, then combines the sub-answers. This can be useful for:\n",
        "- Very long documents (avoiding context window limits)\n",
        "- Parallel processing (faster for many chunks)\n",
        "- Extracting information from each source separately\n",
        "\n",
        "**Trade-off**: Multiple LLM calls increase cost and may miss connections between chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3d2594b7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't have specific information on the types of leaks that might incur a penalty for a Data Fiduciary based on the document provided. The document focuses more on the general responsibilities and obligations of Data Fiduciaries regarding personal data processing. If you need information on common types of data breaches that could lead to penalties for Data Fiduciaries under typical data protection regulations, I can provide a general overview. Let me know if you would like me to do so.\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "deeee38f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In addition to the risk of penalties for personal data breaches, Data Fiduciaries may also face penalties for non-compliance with the provisions outlined in the Data Protection Act. This includes failing to obtain necessary information related to the personal data of Data Principals and its processing, not establishing an effective grievance redressal mechanism for Data Principals, and not responding within the specified period to grievances raised by Data Principals. Data Fiduciaries must also ensure that Data Principals can make requests for the erasure of their personal data in a specified manner. Failure to adhere to these requirements may result in penalties, fines, or legal actions as prescribed by the relevant data protection laws and regulations.'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"refine\"\n",
        ")\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84451da4",
      "metadata": {},
      "source": [
        "**Observation**: The map-reduce approach produced a less specific answer!\n",
        "\n",
        "The model admits it doesn't have \"specific information\" about leak types that incur penalties. This happens because:\n",
        "1. Each chunk was processed **independently** (map step)\n",
        "2. The reduce step couldn't fully reconstruct the comprehensive answer\n",
        "3. Context connections between chunks were lost\n",
        "\n",
        "**Lesson**: The \"stuff\" strategy (default) often works better for questions requiring synthesis across multiple chunks.\n",
        "\n",
        "---\n",
        "\n",
        "### Refine Chain Type\n",
        "\n",
        "The refine approach iteratively improves the answer as it processes each chunk:\n",
        "1. Generate initial answer from Chunk 1\n",
        "2. Refine with Chunk 2\n",
        "3. Further refine with Chunk 3\n",
        "... and so on\n",
        "\n",
        "This can produce more comprehensive, nuanced answers that incorporate details from all chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072570bb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3325a30a",
      "metadata": {},
      "source": [
        "**Observation**: The refine approach provided a more comprehensive answer!\n",
        "\n",
        "The iterative refinement process:\n",
        "1. Started with initial penalties (personal data breaches)\n",
        "2. Added more details as it processed subsequent chunks\n",
        "3. Produced a longer, more detailed response covering multiple non-compliance scenarios\n",
        "\n",
        "**Comparison Summary**:\n",
        "\n",
        "| Chain Type | Answer Quality | Cost | Speed | Use Case |\n",
        "|-----------|----------------|------|-------|----------|\n",
        "| **stuff** | âœ… Best for synthesis | $ | âš¡ Fast | Default choice for most questions |\n",
        "| **map_reduce** | âŒ Lost context | $$$ | âš¡ Fast (parallel) | Very long documents, extractive QA |\n",
        "| **refine** | âœ… Comprehensive | $$ | ðŸŒ Slow (sequential) | Nuanced questions, detailed analysis |\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Best Practices for RAG-based QA\n",
        "\n",
        "### Production Checklist\n",
        "\n",
        "1. **Document Loading Quality** (from `1-DocumentLoading.ipynb`)\n",
        "   - Use appropriate loaders (`PyPDFLoader`, `YoutubeAudioLoader`, etc.)\n",
        "   - Preserve metadata for citation and filtering\n",
        "   - Validate text extraction quality\n",
        "\n",
        "2. **Vector Store Management**\n",
        "   - Use consistent embedding models across loading and querying\n",
        "   - Monitor collection size and quality\n",
        "   - Implement versioning for document updates\n",
        "\n",
        "3. **Prompt Engineering**\n",
        "   - Explicitly instruct models to say \"I don't know\" when uncertain\n",
        "   - Set answer length limits to control verbosity\n",
        "   - Include format requirements in the prompt\n",
        "\n",
        "4. **Chain Type Selection**\n",
        "   - Start with \"stuff\" (default) for most use cases\n",
        "   - Use \"refine\" for questions requiring comprehensive answers\n",
        "   - Use \"map_reduce\" only when context window is a constraint\n",
        "\n",
        "5. **Source Attribution**\n",
        "   - Always return source documents for citation\n",
        "   - Display page numbers and source files to users\n",
        "   - Log which sources influenced each answer (audit trail)\n",
        "\n",
        "6. **Evaluation and Monitoring**\n",
        "   - Test retrieval quality independently of generation\n",
        "   - Monitor answer accuracy against ground truth\n",
        "   - Track cost (LLM calls) and latency for each chain type\n",
        "\n",
        "### Common Pitfalls\n",
        "\n",
        "âŒ **Mismatched embedding models**: Using different models for indexing vs. querying  \n",
        "âœ… **Solution**: Store embedding model version in vector store metadata\n",
        "\n",
        "âŒ **Lost metadata**: Chunking documents without preserving source information  \n",
        "âœ… **Solution**: Use proper document loaders that maintain metadata inheritance\n",
        "\n",
        "âŒ **Over-retrieval**: Fetching too many chunks exceeds context window  \n",
        "âœ… **Solution**: Tune `k` parameter, use smaller chunks, or switch to map_reduce\n",
        "\n",
        "âŒ **Hallucination**: LLM makes up answers not in retrieved context  \n",
        "âœ… **Solution**: Add \"say I don't know\" instructions to prompt, use temperature=0\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "The complete RAG pipeline demonstrated here can be extended with:\n",
        "- **Hybrid search**: Combine vector similarity with keyword matching\n",
        "- **Re-ranking**: Re-score retrieved chunks for better relevance\n",
        "- **Multi-query**: Generate variations of user questions for better recall\n",
        "- **Conversational memory**: Add chat history for follow-up questions\n",
        "- **Streaming**: Stream responses token-by-token for better UX"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

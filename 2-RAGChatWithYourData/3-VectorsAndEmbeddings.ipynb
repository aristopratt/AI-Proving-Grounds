{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2437db65",
      "metadata": {},
      "source": [
        "# Vectors and Embeddings - Semantic Search Foundation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **vector embeddings** and **semantic search**, the core technology behind RAG systems. Key topics:\n",
        "\n",
        "1. **Embeddings Fundamentals** - Converting text to numerical vectors\n",
        "2. **Semantic Similarity** - Measuring meaning, not just keywords\n",
        "3. **Vector Databases** - Efficient storage and retrieval\n",
        "4. **Similarity Search** - Finding relevant documents by meaning\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "### The Problem with Keyword Search\n",
        "- **\"dogs\" vs \"canines\"**: Same meaning, different words → keyword search fails\n",
        "- **Synonyms**: Traditional search can't understand semantic equivalence\n",
        "- **Context**: Word meaning depends on surrounding text\n",
        "\n",
        "### The Solution: Embeddings\n",
        "- **Semantic vectors**: Represent meaning as numerical coordinates\n",
        "- **Similar meanings** → similar vectors → high similarity score\n",
        "- **Scalable search**: Find relevant content in milliseconds from millions of documents\n",
        "\n",
        "## Real-World Applications\n",
        "\n",
        "| Use Case | How Embeddings Help |\n",
        "|----------|---------------------|\n",
        "| **RAG Systems** | Find relevant context for LLM |\n",
        "| **Semantic Search** | \"Find documents about X\" (not just containing \"X\") |\n",
        "| **Recommendation** | \"More like this\" based on content |\n",
        "| **Clustering** | Group similar documents automatically |\n",
        "| **Anomaly Detection** | Find unusual/outlier content |\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Embedding\n",
        "A **dense vector** (typically 1536 dimensions for OpenAI) representing semantic meaning:\n",
        "```python\n",
        "\"I like dogs\" → [0.023, -0.145, 0.892, ..., 0.034]  # 1536 numbers\n",
        "```\n",
        "\n",
        "### Cosine Similarity\n",
        "Measures how similar two vectors are (0 = unrelated, 1 = identical):\n",
        "```python\n",
        "similarity(\"dogs\", \"canines\") = 0.92  # High - same meaning\n",
        "similarity(\"dogs\", \"weather\") = 0.71  # Low - unrelated\n",
        "```\n",
        "\n",
        "### Vector Database\n",
        "Specialized database for:\n",
        "- Storing embeddings efficiently\n",
        "- Fast similarity search (ANN - Approximate Nearest Neighbors)\n",
        "- Metadata filtering\n",
        "\n",
        "## Technologies Used\n",
        "\n",
        "- **OpenAI Embeddings**: `text-embedding-ada-002` model (1536 dimensions)\n",
        "- **ChromaDB**: Open-source vector database\n",
        "- **NumPy**: Similarity calculations\n",
        "- **LangChain**: Integration layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10562a74",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Standard imports plus tiktoken for token counting (embeddings are billed per token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "533778a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken  # OpenAI's tokenizer for accurate token counting\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# Load environment variables from .env file (contains OPENAI_API_KEY)\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ead56ff",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1: Load Source Documents\n",
        "\n",
        "Loading the same PDF we used in previous notebooks (Indian Data Protection Act).\n",
        "\n",
        "**Why This Document?**\n",
        "- Real-world legal text (complex, multi-page)\n",
        "- Demonstrates retrieval from long documents\n",
        "- Shows how embeddings find relevant sections in legislation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04c58e9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF - Digital Personal Data Protection Act (India, 2023)\n",
        "loaders = [\n",
        "    PyPDFLoader(\"./99-DPDPA.pdf\")  # Real-world legal document\n",
        "]\n",
        "\n",
        "# Aggregate all pages into single list\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())  # Each page becomes a Document object"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd14705d",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Split Documents into Chunks\n",
        "\n",
        "Before creating embeddings, we need to split the document into appropriately-sized chunks.\n",
        "\n",
        "**Why Split?**\n",
        "- **Context window limits**: LLMs have token limits (4K-128K)\n",
        "- **Relevance**: Smaller chunks = more precise retrieval\n",
        "- **Cost**: Embedding entire documents is expensive and less effective\n",
        "\n",
        "**Chunk Size: 1500 characters**\n",
        "- ~300-400 tokens\n",
        "- 1-2 paragraphs of text\n",
        "- Balance: specific enough to be relevant, large enough for context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "92950b79",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configure text splitter for optimal chunk sizes\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,      # ~300-400 tokens, 1-2 paragraphs\n",
        "    chunk_overlap=150     # 10% overlap preserves context across boundaries\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66650334",
      "metadata": {},
      "source": [
        "## Execute Splitting\n",
        "\n",
        "Splits the PDF into 55 chunks. Each chunk will get its own embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f449ab80",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split documents into chunks\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Total chunks (each will become a vector in the database)\n",
        "len(splits)  # Output: 55 chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e638f8",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Initialize Embedding Model\n",
        "\n",
        "OpenAI's embedding model converts text to 1536-dimensional vectors.\n",
        "\n",
        "### Model: `text-embedding-ada-002`\n",
        "\n",
        "**Specifications**:\n",
        "- **Dimensions**: 1536\n",
        "- **Max Input**: 8,191 tokens per request\n",
        "- **Cost**: $0.0001 per 1K tokens (~$0.0055 for 55 chunks)\n",
        "- **Performance**: Optimized for semantic search\n",
        "\n",
        "**What Happens When You Call `embed_query()`?**\n",
        "1. Text → Tokens (tokenization)\n",
        "2. Tokens → API (HTTP request to OpenAI)\n",
        "3. API → 1536 floating-point numbers\n",
        "4. Numbers represent semantic meaning in high-dimensional space\n",
        "\n",
        "**Note**: Deprecation warning is expected - newer package exists but functionality is identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "760ef74c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/l8/yvtt6g7s4mg361124qs02mx40000gn/T/ipykernel_63314/2848396342.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding = OpenAIEmbeddings()\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize OpenAI embedding model (text-embedding-ada-002)\n",
        "# This will convert text strings into 1536-dimensional vectors\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# Note: Deprecation warning is expected - using langchain_community version\n",
        "# Functionality is identical to newer langchain_openai package"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b8fefca",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Embeddings Experiment: Semantic Similarity\n",
        "\n",
        "Let's demonstrate how embeddings capture **meaning, not just keywords**.\n",
        "\n",
        "### Test Sentences\n",
        "\n",
        "1. **\"i like dogs\"** - Base sentence\n",
        "2. **\"i like canines\"** - Same meaning, different word (synonym)\n",
        "3. **\"the weather is ugly outside\"** - Completely unrelated\n",
        "\n",
        "### Expected Results\n",
        "\n",
        "**High similarity**: sentences 1 & 2 (dogs = canines)  \n",
        "**Low similarity**: sentence 1 & 3 (unrelated topics)  \n",
        "**Low similarity**: sentence 2 & 3 (unrelated topics)\n",
        "\n",
        "This proves embeddings understand **semantics**, not just lexical overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0d64aa6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test sentences for semantic similarity comparison\n",
        "sentence1 = \"i like dogs\"                    # Base sentence\n",
        "sentence2 = \"i like canines\"                 # Synonym - same meaning\n",
        "sentence3 = \"the weather is ugly outside\"    # Unrelated - different topic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e25960",
      "metadata": {},
      "source": [
        "## Generate Embeddings\n",
        "\n",
        "Each sentence becomes a 1536-dimensional vector. This makes 3 API calls to OpenAI.\n",
        "\n",
        "**Cost**: ~3 API calls × ~10 tokens each = ~$0.000003 (negligible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fb4bf93e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings (each is 1536-dimensional vector)\n",
        "embedding1 = embedding.embed_query(sentence1)  # [0.023, -0.145, ..., 0.034]\n",
        "embedding2 = embedding.embed_query(sentence2)  # [0.019, -0.140, ..., 0.029]\n",
        "embedding3 = embedding.embed_query(sentence3)  # [-0.052, 0.201, ..., -0.078]\n",
        "\n",
        "# Each embedding is a list of 1536 floating-point numbers\n",
        "# Similar meanings → similar vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d11b8e",
      "metadata": {},
      "source": [
        "## Calculate Cosine Similarity\n",
        "\n",
        "Using NumPy's dot product to measure similarity between vectors.\n",
        "\n",
        "### Similarity Score Interpretation\n",
        "\n",
        "- **0.96**: \"dogs\" vs \"canines\" → **Very high** (synonyms!)\n",
        "- **0.77**: \"dogs\" vs \"weather\" → **Low** (unrelated)\n",
        "- **0.76**: \"canines\" vs \"weather\" → **Low** (unrelated)\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "The embedding model **correctly identifies** that \"dogs\" and \"canines\" are semantically similar (0.96), despite being different words. This is the power of semantic embeddings!\n",
        "\n",
        "**Note**: Scores range from -1 (opposite) to 1 (identical). Values >0.9 indicate strong semantic similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1cac8561",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7590539714454777\n",
            "0.7702031204123153\n",
            "0.9631510802407718\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate cosine similarity using dot product\n",
        "# (For normalized vectors, dot product = cosine similarity)\n",
        "\n",
        "print(np.dot(embedding2, embedding3))  # \"canines\" vs \"weather\" = 0.76 (LOW - unrelated)\n",
        "print(np.dot(embedding1, embedding3))  # \"dogs\" vs \"weather\" = 0.77 (LOW - unrelated)\n",
        "print(np.dot(embedding1, embedding2))  # \"dogs\" vs \"canines\" = 0.96 (HIGH - synonyms!)\n",
        "\n",
        "# Key insight: \"dogs\" and \"canines\" are semantically similar (0.96)\n",
        "# despite being different words - this is semantic search!   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa13800",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Vector Database Setup\n",
        "\n",
        "Now we'll store all document chunks in a **vector database** for efficient similarity search.\n",
        "\n",
        "### Why ChromaDB?\n",
        "\n",
        "| Feature | Benefit |\n",
        "|---------|---------|\n",
        "| **Open Source** | Free, self-hosted |\n",
        "| **Embedded** | Runs in-process (no separate server) |\n",
        "| **Persistent** | Save to disk, reload later |\n",
        "| **Fast** | Approximate Nearest Neighbors (ANN) |\n",
        "| **Metadata** | Filter by document properties |\n",
        "\n",
        "### Alternative Vector DBs\n",
        "\n",
        "- **Pinecone**: Managed, cloud-hosted (paid)\n",
        "- **Weaviate**: Open-source, GraphQL API\n",
        "- **Qdrant**: Rust-based, high performance\n",
        "- **FAISS**: Facebook's library (no persistence by default)\n",
        "\n",
        "For this demo, ChromaDB is perfect: simple, local, and persistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "84b03140",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Import ChromaDB - embedded vector database\n",
        "# This will store embeddings and enable fast similarity search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58bc07bd",
      "metadata": {},
      "source": [
        "## Clean Up Old Database\n",
        "\n",
        "Remove any existing database to start fresh. In production, you'd only do this when rebuilding the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d89fff7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define where to persist the vector database\n",
        "persist_directory = 'docs/chroma/'\n",
        "\n",
        "# Clean up: remove old database files (if any)\n",
        "!rm -rf ./docs/chroma\n",
        "\n",
        "# In production: only rebuild when documents change or you need to update embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3bec87",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Create Vector Database\n",
        "\n",
        "This is where the magic happens! This single command:\n",
        "\n",
        "1. **Embeds all 55 chunks** (55 API calls to OpenAI)\n",
        "2. **Stores vectors** in ChromaDB\n",
        "3. **Builds search index** (ANN data structures)\n",
        "4. **Saves to disk** (persistent storage)\n",
        "\n",
        "### What's Happening Behind the Scenes\n",
        "\n",
        "```python\n",
        "for chunk in splits:  # 55 chunks\n",
        "    # 1. Generate embedding\n",
        "    vector = openai.embeddings.create(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        input=chunk.page_content\n",
        "    )\n",
        "    \n",
        "    # 2. Store in database\n",
        "    chroma.add(\n",
        "        vector=vector,\n",
        "        metadata=chunk.metadata,  # page number, source, etc.\n",
        "        content=chunk.page_content\n",
        "    )\n",
        "```\n",
        "\n",
        "### Cost Breakdown\n",
        "\n",
        "- **55 chunks** × ~300 tokens each = ~16,500 tokens\n",
        "- **Price**: $0.0001 per 1K tokens\n",
        "- **Total**: ~$0.0017 (less than a quarter of a cent!)\n",
        "\n",
        "### Time\n",
        "\n",
        "- Approximately 10-30 seconds for 55 embeddings\n",
        "- Most time spent waiting for API responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c024bdd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create vector database from document chunks\n",
        "# This will:\n",
        "# 1. Generate embeddings for all 55 chunks (55 API calls)\n",
        "# 2. Store vectors + metadata in ChromaDB\n",
        "# 3. Save to disk for future use\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,               # Our 55 document chunks\n",
        "    embedding=embedding,            # OpenAI embedding function\n",
        "    persist_directory=persist_directory  # Save to ./docs/chroma/\n",
        ")\n",
        "\n",
        "# Cost: ~16,500 tokens = ~$0.0017 (less than 1/4 of a cent!)\n",
        "# Time: ~10-30 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a609136",
      "metadata": {},
      "source": [
        "## Verify Database\n",
        "\n",
        "Confirm all 55 chunks were successfully embedded and stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d18fb8ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n"
          ]
        }
      ],
      "source": [
        "# Verify: count total vectors in database\n",
        "print(vectordb._collection.count())  # Should output: 55\n",
        "\n",
        "# This confirms all chunks were successfully embedded and stored"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ad4e54",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Semantic Search in Action!\n",
        "\n",
        "Now we can search the document **by meaning**, not just keywords.\n",
        "\n",
        "### Query\n",
        "\n",
        "**\"is there any rules for a data principal?\"**\n",
        "\n",
        "This question uses natural language:\n",
        "- \"rules\" (the document uses \"rights\" and \"obligations\")\n",
        "- \"data principal\" (specific legal term in the document)\n",
        "\n",
        "### How Similarity Search Works\n",
        "\n",
        "1. **Embed the query** (1 API call)\n",
        "   ```python\n",
        "   query_vector = embedding.embed_query(question)\n",
        "   ```\n",
        "\n",
        "2. **Find similar vectors** (fast, local computation)\n",
        "   ```python\n",
        "   # Compare query_vector to all 55 stored vectors\n",
        "   # Return top-k most similar (k=3)\n",
        "   ```\n",
        "\n",
        "3. **Return original text** of most similar chunks\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- **k=3**: Return top 3 most relevant chunks\n",
        "- Could adjust based on needs (k=1 for single best match, k=10 for more context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9aebac4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Natural language question about the document\n",
        "question = \"is there any rules for a data principal?\"\n",
        "\n",
        "# Note: Question uses \"rules\" but document uses \"rights\", \"obligations\"\n",
        "# Semantic search will still find relevant sections!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf949ba",
      "metadata": {},
      "source": [
        "## Execute Similarity Search\n",
        "\n",
        "The search returns 3 most relevant chunks. Let's examine the top result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0d3d77e1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'section 6 and section 13; and\\n (iii) the manner in which the Data Principal may make a complaint to the Board,\\nin such manner and as may be prescribed.\\nIllustration.\\nX, an individual, opens a bank account using the mobile app or website of Y , a bank.\\nTo complete the Know-Your-Customer requirements under law for opening of bank account,\\nX opts for processing of her personal data by Y in a live, video-based customer identification\\nprocess. Y shall accompany or precede the request for the personal data with notice to X,\\ndescribing the personal data and the purpose of its processing.\\n(2) Where a Data Principal has given her consent for the processing of her personal\\ndata before the date of commencement of this Act,—\\n(a) the Data Fiduciary shall, as soon as it is reasonably practicable, give to the\\nData Principal a notice informing her,––\\n(i) the personal data and the purpose for which the same has been\\nprocessed;\\n  (ii) the manner in which she may exercise her rights under sub-section (4)\\nof section 6 and section 13; and\\n (iii) the manner in which the Data Principal may make a complaint to the\\nBoard,\\nin such manner and as may be prescribed.\\nGrounds for\\nprocessing\\npersonal data.\\nNotice.'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform similarity search\n",
        "# 1. Embeds the question (1 API call)\n",
        "# 2. Compares to all 55 stored vectors (fast, local)\n",
        "# 3. Returns top 3 most similar chunks\n",
        "docs = vectordb.similarity_search(question, k=3)\n",
        "\n",
        "# Verify we got 3 results\n",
        "len(docs)  # Output: 3\n",
        "\n",
        "# Display the most relevant chunk (highest similarity score)\n",
        "docs[0].page_content\n",
        "\n",
        "# This text is the most semantically similar to our question!\n",
        "# Notice: it contains \"Data Principal\", \"rights\", \"notice\" - all relevant to \"rules\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d4b5e4a",
      "metadata": {},
      "source": [
        "### Analysis of Result\n",
        "\n",
        "The top result is **highly relevant**:\n",
        "- Contains \"Data Principal\" (exact term from query)\n",
        "- Describes \"rights\" under sections 6 and 13 (the \"rules\" we asked about)\n",
        "- Includes \"notice\" requirements (procedural rules)\n",
        "- Shows concrete example (bank account KYC)\n",
        "\n",
        "**This proves semantic search works!** The query used \"rules\" but the document text uses \"rights\", \"consent\", \"notice\" - yet the system found the correct section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e09dbf",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Persist Database\n",
        "\n",
        "Save the database to disk so we can reload it later without re-embedding.\n",
        "\n",
        "**Note**: Newer ChromaDB versions auto-persist, so this command is optional but harmless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ffd8df8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/l8/yvtt6g7s4mg361124qs02mx40000gn/T/ipykernel_63314/3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectordb.persist()\n"
          ]
        }
      ],
      "source": [
        "# Persist database to disk (save for future use)\n",
        "vectordb.persist()\n",
        "\n",
        "# Note: Newer ChromaDB versions auto-persist, so this is optional\n",
        "# But calling it explicitly doesn't hurt and ensures compatibility\n",
        "\n",
        "# Next time: load existing database instead of re-embedding\n",
        "# vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2964a33",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: What We Accomplished\n",
        "\n",
        "### 1. Embeddings Fundamentals ✅\n",
        "- Learned how text becomes numerical vectors (1536 dimensions)\n",
        "- Demonstrated semantic similarity: \"dogs\" vs \"canines\" = 0.96\n",
        "- Proved embeddings understand meaning, not just keywords\n",
        "\n",
        "### 2. Vector Database ✅\n",
        "- Created ChromaDB with 55 document chunks\n",
        "- Each chunk embedded and indexed for fast search\n",
        "- Persistent storage for future use\n",
        "\n",
        "### 3. Semantic Search ✅\n",
        "- Asked natural language question\n",
        "- Retrieved relevant sections despite vocabulary mismatch\n",
        "- \"rules\" → found text containing \"rights\", \"notice\", \"consent\"\n",
        "\n",
        "---\n",
        "\n",
        "## Production Patterns\n",
        "\n",
        "### 1. Loading Existing Database\n",
        "\n",
        "```python\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize embedding function\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# Load existing database (no re-embedding needed!)\n",
        "vectordb = Chroma(\n",
        "    persist_directory='docs/chroma/',\n",
        "    embedding_function=embedding\n",
        ")\n",
        "\n",
        "# Ready to search immediately\n",
        "results = vectordb.similarity_search(\"your question\", k=3)\n",
        "```\n",
        "\n",
        "### 2. Incremental Updates\n",
        "\n",
        "```python\n",
        "# Add new documents without rebuilding entire index\n",
        "new_docs = load_new_documents()\n",
        "new_splits = text_splitter.split_documents(new_docs)\n",
        "\n",
        "# Add to existing database\n",
        "vectordb.add_documents(new_splits)\n",
        "```\n",
        "\n",
        "### 3. Metadata Filtering\n",
        "\n",
        "```python\n",
        "# Search with filters (e.g., only specific document sections)\n",
        "results = vectordb.similarity_search(\n",
        "    \"your question\",\n",
        "    k=3,\n",
        "    filter={\"source\": \"section_6\"}  # Only search in section 6\n",
        ")\n",
        "```\n",
        "\n",
        "### 4. Similarity Scores\n",
        "\n",
        "```python\n",
        "# Get similarity scores along with documents\n",
        "results = vectordb.similarity_search_with_score(\"your question\", k=3)\n",
        "\n",
        "for doc, score in results:\n",
        "    print(f\"Score: {score:.3f}\")\n",
        "    print(f\"Content: {doc.page_content[:100]}...\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Cost Analysis\n",
        "\n",
        "### Embedding Costs\n",
        "\n",
        "| Operation | Tokens | Cost | Frequency |\n",
        "|-----------|--------|------|-----------|\n",
        "| **Initial indexing** | 16,500 | $0.0017 | Once (or on rebuild) |\n",
        "| **Query** | ~10 | $0.000001 | Per search |\n",
        "| **Add 1 document** | ~300 | $0.00003 | As needed |\n",
        "\n",
        "### Cost Optimization Strategies\n",
        "\n",
        "1. **Batch Processing**: Embed multiple documents in single API call\n",
        "2. **Caching**: Reuse existing embeddings, don't re-embed unchanged content\n",
        "3. **Incremental Updates**: Only embed new/changed documents\n",
        "4. **Local Embeddings**: Use open-source models (e.g., sentence-transformers) for sensitive data\n",
        "\n",
        "---\n",
        "\n",
        "## Advanced Techniques\n",
        "\n",
        "### 1. Hybrid Search (Keyword + Semantic)\n",
        "\n",
        "```python\n",
        "# Combine keyword search (BM25) with semantic search\n",
        "# Best of both worlds: exact matches + semantic understanding\n",
        "\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# Keyword retriever\n",
        "bm25_retriever = BM25Retriever.from_documents(splits)\n",
        "\n",
        "# Semantic retriever\n",
        "vector_retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Combine both\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, vector_retriever],\n",
        "    weights=[0.3, 0.7]  # 30% keyword, 70% semantic\n",
        ")\n",
        "\n",
        "results = ensemble_retriever.get_relevant_documents(\"your question\")\n",
        "```\n",
        "\n",
        "### 2. Maximal Marginal Relevance (MMR)\n",
        "\n",
        "```python\n",
        "# Avoid returning duplicate/similar results\n",
        "# MMR balances relevance with diversity\n",
        "\n",
        "results = vectordb.max_marginal_relevance_search(\n",
        "    \"your question\",\n",
        "    k=3,\n",
        "    fetch_k=10,  # Fetch 10 candidates, return diverse 3\n",
        "    lambda_mult=0.5  # Balance relevance (1.0) vs diversity (0.0)\n",
        ")\n",
        "```\n",
        "\n",
        "### 3. Self-Query Retriever\n",
        "\n",
        "```python\n",
        "# Let LLM extract filters from natural language\n",
        "# \"Find documents from 2023 about privacy\" \n",
        "# → filter: {year: 2023, topic: \"privacy\"}\n",
        "\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "\n",
        "metadata_fields = [\n",
        "    AttributeInfo(name=\"source\", description=\"Document filename\", type=\"string\"),\n",
        "    AttributeInfo(name=\"page\", description=\"Page number\", type=\"integer\")\n",
        "]\n",
        "\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    llm=your_llm,\n",
        "    vectorstore=vectordb,\n",
        "    document_contents=\"Legal documents about data privacy\",\n",
        "    metadata_field_info=metadata_fields\n",
        ")\n",
        "\n",
        "# Natural language query with implicit filter\n",
        "results = retriever.get_relevant_documents(\n",
        "    \"What does page 5 say about consent?\"  \n",
        "    # Auto-extracts: filter={page: 5}\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Common Pitfalls\n",
        "\n",
        "### 1. Chunk Size Too Large\n",
        "**Problem**: Large chunks reduce retrieval precision  \n",
        "**Solution**: Keep chunks 300-800 tokens (1-3 paragraphs)\n",
        "\n",
        "### 2. No Overlap\n",
        "**Problem**: Context lost at chunk boundaries  \n",
        "**Solution**: Use 10-20% overlap\n",
        "\n",
        "### 3. Forgetting to Persist\n",
        "**Problem**: Database lost on restart  \n",
        "**Solution**: Always specify `persist_directory`\n",
        "\n",
        "### 4. Wrong Embedding Model\n",
        "**Problem**: Using different embeddings for indexing vs querying  \n",
        "**Solution**: Always use same embedding function\n",
        "\n",
        "### 5. Ignoring Metadata\n",
        "**Problem**: Can't filter search by document properties  \n",
        "**Solution**: Preserve metadata during splitting\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "### In This Module\n",
        "- **Notebook 4**: Question Answering over Documents (RAG)\n",
        "- **Notebook 5**: Chat with your data (conversational RAG)\n",
        "\n",
        "### Production Deployment\n",
        "1. **Scale**: Use managed vector DB (Pinecone, Weaviate)\n",
        "2. **Monitor**: Track search quality, latency, cost\n",
        "3. **Optimize**: Experiment with chunk sizes, overlap\n",
        "4. **Enhance**: Add hybrid search, metadata filtering\n",
        "5. **Secure**: Implement access controls, audit logging\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "✅ **Embeddings** represent meaning as vectors  \n",
        "✅ **Semantic similarity** captures synonyms and context  \n",
        "✅ **Vector databases** enable fast similarity search  \n",
        "✅ **ChromaDB** provides persistent, embedded storage  \n",
        "✅ **RAG foundation** is now complete - ready for Q&A!\n",
        "\n",
        "**You now understand the core technology behind modern RAG systems!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd8a388",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670dd2ad",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

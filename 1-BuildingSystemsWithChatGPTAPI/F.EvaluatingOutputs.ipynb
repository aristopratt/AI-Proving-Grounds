{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d71ddb9",
      "metadata": {},
      "source": [
        "# Evaluating LLM Outputs with LLM-as-Judge\n",
        "\n",
        "## Overview\n",
        "\n",
        "Production systems must **validate LLM outputs** before sending to users. This notebook demonstrates the **LLM-as-judge pattern**: using an LLM to evaluate another LLM's responses.\n",
        "\n",
        "## Why Evaluation Matters\n",
        "\n",
        "### Quality Assurance\n",
        "- **Accuracy**: Does response match facts?\n",
        "- **Completeness**: Does it answer the question?\n",
        "- **Relevance**: Is it on-topic?\n",
        "- **Safety**: No harmful/inappropriate content?\n",
        "\n",
        "### Use Cases\n",
        "1. **Pre-deployment**: Test prompt changes\n",
        "2. **Production**: Real-time quality gates\n",
        "3. **Monitoring**: Track quality metrics over time\n",
        "4. **A/B Testing**: Compare prompt variants\n",
        "\n",
        "## LLM-as-Judge Pattern\n",
        "\n",
        "**Concept**: Use a separate LLM call to evaluate the primary LLM's output\n",
        "\n",
        "**Benefits**:\n",
        "- Automated at scale (no human review)\n",
        "- Consistent criteria\n",
        "- Cost-effective (~$0.0001 per evaluation)\n",
        "- Fast (<500ms)\n",
        "\n",
        "**Limitations**:\n",
        "- Not 100% accurate (LLMs can misjudge)\n",
        "- Requires clear evaluation criteria\n",
        "- Best for binary/simple judgments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b745abd",
      "metadata": {},
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f7ebf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27601b4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Layer 1: Content Moderation\n",
        "\n",
        "Before evaluating quality, ensure the response passes content safety checks using OpenAI's Moderation API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16aa7690",
      "metadata": {},
      "outputs": [],
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "def get_completion_from_messages(\n",
        "    messages,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    max_tokens=500,\n",
        "):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b05c48",
      "metadata": {},
      "source": [
        "**Expected Result**: `flagged=False` (no policy violations)\n",
        "\n",
        "This is a benign customer service response, so it should pass moderation.\n",
        "\n",
        "---\n",
        "\n",
        "## Layer 2: LLM-as-Judge Evaluation\n",
        "\n",
        "### Evaluation Criteria\n",
        "\n",
        "1. **Factual Accuracy**: Does response match product info?\n",
        "2. **Completeness**: Does it answer the user's question?\n",
        "\n",
        "### System Message Design\n",
        "\n",
        "Creates an evaluation LLM with two jobs:\n",
        "- Check facts against source data\n",
        "- Verify question was answered\n",
        "\n",
        "**Output**: Binary Y/N (simple, fast, low-cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29377b51",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_response_to_customer = f\"\"\"\n",
        "The SmartX ProPhone has a 6.1-inch display, 128GB storage, \\\n",
        "12MP dual camera, and 5G. The FotoSnap DSLR Camera \\\n",
        "has a 24.2MP sensor, 1080p video, 3-inch LCD, and \\\n",
        "interchangeable lenses. We have a variety of TVs, including \\\n",
        "the CineView 4K TV with a 55-inch display, 4K resolution, \\\n",
        "HDR, and smart TV features. We also have the SoundMax \\\n",
        "Home Theater system with 5.1 channel, 1000W output, wireless \\\n",
        "subwoofer, and Bluetooth. Do you have any specific questions \\\n",
        "about these products or any other products we offer?\n",
        "\"\"\"\n",
        "response = client.moderations.create(\n",
        "    input=final_response_to_customer\n",
        ")\n",
        "moderation_output = response.results[0]\n",
        "print(moderation_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a5578c",
      "metadata": {},
      "source": [
        "**Expected Output**: `Y` (response is accurate and complete)\n",
        "\n",
        "The response correctly states:\n",
        "- SmartX ProPhone: 6.1-inch, 128GB, 12MP, 5G ✓\n",
        "- FotoSnap DSLR: 24.2MP, 1080p, 3-inch LCD ✓\n",
        "- TVs: CineView 4K, SoundMax Home Theater ✓\n",
        "\n",
        "---\n",
        "\n",
        "## Test Case 2: Irrelevant Response\n",
        "\n",
        "Testing with a nonsensical response to verify the judge catches bad outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4131f6bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = f\"\"\"\n",
        "You are an assistant that evaluates whether \\\n",
        "customer service agent responses sufficiently \\\n",
        "answer customer questions, and also validates that \\\n",
        "all the facts the assistant cites from the product \\\n",
        "information are correct.\n",
        "The product information and user and customer \\\n",
        "service agent messages will be delimited by \\\n",
        "3 backticks, i.e. ```.\n",
        "Respond with a Y or N character, with no punctuation:\n",
        "Y - if the output sufficiently answers the question \\\n",
        "AND the response correctly uses product information\n",
        "N - otherwise\n",
        "\n",
        "Output a single letter only.\n",
        "\"\"\"\n",
        "customer_message = f\"\"\"\n",
        "tell me about the smartx pro phone and \\\n",
        "the fotosnap camera, the dslr one. \\\n",
        "Also tell me about your tvs\"\"\"\n",
        "product_information = \"\"\"{ \"name\": \"SmartX ProPhone\", \"category\": \"Smartphones and Accessories\", \"brand\": \"SmartX\", \"model_number\": \"SX-PP10\", \"warranty\": \"1 year\", \"rating\": 4.6, \"features\": [ \"6.1-inch display\", \"128GB storage\", \"12MP dual camera\", \"5G\" ], \"description\": \"A powerful smartphone with advanced camera features.\", \"price\": 899.99 } { \"name\": \"FotoSnap DSLR Camera\", \"category\": \"Cameras and Camcorders\", \"brand\": \"FotoSnap\", \"model_number\": \"FS-DSLR200\", \"warranty\": \"1 year\", \"rating\": 4.7, \"features\": [ \"24.2MP sensor\", \"1080p video\", \"3-inch LCD\", \"Interchangeable lenses\" ], \"description\": \"Capture stunning photos and videos with this versatile DSLR camera.\", \"price\": 599.99 } { \"name\": \"CineView 4K TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-4K55\", \"warranty\": \"2 years\", \"rating\": 4.8, \"features\": [ \"55-inch display\", \"4K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"A stunning 4K TV with vibrant colors and smart features.\", \"price\": 599.99 } { \"name\": \"SoundMax Home Theater\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"SoundMax\", \"model_number\": \"SM-HT100\", \"warranty\": \"1 year\", \"rating\": 4.4, \"features\": [ \"5.1 channel\", \"1000W output\", \"Wireless subwoofer\", \"Bluetooth\" ], \"description\": \"A powerful home theater system for an immersive audio experience.\", \"price\": 399.99 } { \"name\": \"CineView 8K TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-8K65\", \"warranty\": \"2 years\", \"rating\": 4.9, \"features\": [ \"65-inch display\", \"8K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"Experience the future of television with this stunning 8K TV.\", \"price\": 2999.99 } { \"name\": \"SoundMax Soundbar\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"SoundMax\", \"model_number\": \"SM-SB50\", \"warranty\": \"1 year\", \"rating\": 4.3, \"features\": [ \"2.1 channel\", \"300W output\", \"Wireless subwoofer\", \"Bluetooth\" ], \"description\": \"Upgrade your TV's audio with this sleek and powerful soundbar.\", \"price\": 199.99 } { \"name\": \"CineView OLED TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-OLED55\", \"warranty\": \"2 years\", \"rating\": 4.7, \"features\": [ \"55-inch display\", \"4K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"Experience true blacks and vibrant colors with this OLED TV.\", \"price\": 1499.99 }\"\"\"\n",
        "q_a_pair = f\"\"\"\n",
        "Customer message: ```{customer_message}```\n",
        "Product information: ```{product_information}```\n",
        "Agent response: ```{final_response_to_customer}```\n",
        "\n",
        "Does the response use the retrieved information correctly?\n",
        "Does the response sufficiently answer the question\n",
        "\n",
        "Output Y or N\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {'role': 'system', 'content': system_message},\n",
        "    {'role': 'user', 'content': q_a_pair}\n",
        "]\n",
        "\n",
        "response = get_completion_from_messages(messages, max_tokens=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620a801b",
      "metadata": {},
      "source": [
        "**Expected Output**: `N` (fails both criteria)\n",
        "\n",
        "\"Life is like a box of chocolates\" doesn't:\n",
        "- Use product information ✗\n",
        "- Answer the question ✗\n",
        "\n",
        "**Key Insight**: The evaluator correctly rejects irrelevant responses.\n",
        "\n",
        "---\n",
        "\n",
        "## Production Implementation\n",
        "\n",
        "### Complete Evaluation Pipeline\n",
        "\n",
        "```python\n",
        "def evaluate_and_send_response(user_query, llm_response, product_data):\n",
        "    \\\"\\\"\\\"\n",
        "    Multi-layer evaluation before sending to user.\n",
        "    Returns: (should_send: bool, reason: str)\n",
        "    \\\"\\\"\\\"\n",
        "    # Layer 1: Content safety\n",
        "    moderation = client.moderations.create(input=llm_response)\n",
        "    if moderation.results[0].flagged:\n",
        "        return False, \"Content policy violation\"\n",
        "    \n",
        "    # Layer 2: Factual accuracy + completeness\n",
        "    eval_prompt = f\\\"\\\"\\\"\n",
        "    Evaluate this response:\n",
        "    User: {user_query}\n",
        "    Product Data: {product_data}\n",
        "    Response: {llm_response}\n",
        "    \n",
        "    Check: Facts correct? Question answered?\n",
        "    Output: Y or N\n",
        "    \\\"\\\"\\\"\n",
        "    \n",
        "    judgment = get_completion(eval_prompt, max_tokens=1)\n",
        "    \n",
        "    if judgment != 'Y':\n",
        "        log_failed_response(user_query, llm_response, \"Failed evaluation\")\n",
        "        return False, \"Quality check failed\"\n",
        "    \n",
        "    # Passed all checks\n",
        "    return True, None\n",
        "\n",
        "# Usage\n",
        "should_send, reason = evaluate_and_send_response(query, response, data)\n",
        "if should_send:\n",
        "    send_to_user(response)\n",
        "else:\n",
        "    # Fallback or regenerate\n",
        "    send_fallback_response(reason)\n",
        "```\n",
        "\n",
        "### Evaluation Dimensions\n",
        "\n",
        "Expand beyond binary Y/N:\n",
        "\n",
        "```python\n",
        "system_message = \\\"\\\"\\\"\n",
        "Evaluate on 1-5 scale:\n",
        "1. Factual Accuracy (uses data correctly)\n",
        "2. Completeness (answers all parts)\n",
        "3. Relevance (on-topic)\n",
        "4. Tone (professional, friendly)\n",
        "5. Clarity (easy to understand)\n",
        "\n",
        "Output JSON: {\n",
        "    \"accuracy\": 5,\n",
        "    \"completeness\": 4,\n",
        "    \"relevance\": 5,\n",
        "    \"tone\": 5,\n",
        "    \"clarity\": 4,\n",
        "    \"overall_pass\": true\n",
        "}\n",
        "\\\"\\\"\\\"\n",
        "```\n",
        "\n",
        "### Cost Analysis\n",
        "\n",
        "**Per Evaluation**:\n",
        "- Input tokens: ~500 (query + product data + response)\n",
        "- Output tokens: ~50 (judgment + reasoning)\n",
        "- Cost: ~$0.0003 (gpt-3.5-turbo)\n",
        "\n",
        "**At Scale** (10K evaluations/day):\n",
        "- Daily cost: $3\n",
        "- Monthly cost: $90\n",
        "\n",
        "**Worth It If**:\n",
        "- Prevents one bad customer interaction (support ticket = $20-50)\n",
        "- ROI positive after preventing ~2-3 escalations/month\n",
        "\n",
        "### Automated Quality Metrics\n",
        "\n",
        "```python\n",
        "class EvaluationMetrics:\n",
        "    def __init__(self):\n",
        "        self.total = 0\n",
        "        self.passed = 0\n",
        "        self.failed = 0\n",
        "        self.fail_reasons = defaultdict(int)\n",
        "    \n",
        "    def record(self, passed, reason=None):\n",
        "        self.total += 1\n",
        "        if passed:\n",
        "            self.passed += 1\n",
        "        else:\n",
        "            self.failed += 1\n",
        "            self.fail_reasons[reason] += 1\n",
        "    \n",
        "    def report(self):\n",
        "        return {\n",
        "            \"pass_rate\": self.passed / self.total,\n",
        "            \"total_evaluated\": self.total,\n",
        "            \"common_failures\": dict(self.fail_reasons.most_common(5))\n",
        "        }\n",
        "\n",
        "# Track over time\n",
        "metrics = EvaluationMetrics()\n",
        "# ...after each evaluation...\n",
        "metrics.record(judgment == 'Y')\n",
        "\n",
        "# Weekly report\n",
        "print(metrics.report())\n",
        "# Output: {\"pass_rate\": 0.94, \"total_evaluated\": 1000, ...}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Advanced Evaluation Patterns\n",
        "\n",
        "### 1. Multi-Model Consensus\n",
        "\n",
        "Use multiple models as judges, take majority vote:\n",
        "\n",
        "```python\n",
        "def consensus_evaluation(response, criteria):\n",
        "    judges = [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4o\"]\n",
        "    votes = []\n",
        "    \n",
        "    for model in judges:\n",
        "        judgment = evaluate_with_model(response, criteria, model)\n",
        "        votes.append(judgment)\n",
        "    \n",
        "    # Majority vote\n",
        "    return Counter(votes).most_common(1)[0][0]\n",
        "```\n",
        "\n",
        "### 2. Reference-Based Evaluation\n",
        "\n",
        "Compare against known good responses:\n",
        "\n",
        "```python\n",
        "system_message = \\\"\\\"\\\"\n",
        "Compare the agent's response to this reference answer:\n",
        "\n",
        "Reference: {reference_answer}\n",
        "Agent Response: {agent_response}\n",
        "\n",
        "Rate similarity (1-5) considering:\n",
        "- Same facts mentioned\n",
        "- Similar level of detail\n",
        "- Comparable tone\n",
        "\n",
        "Output: Score and reasoning\n",
        "\\\"\\\"\\\"\n",
        "```\n",
        "\n",
        "### 3. Chain-of-Thought Evaluation\n",
        "\n",
        "Ask evaluator to explain reasoning:\n",
        "\n",
        "```python\n",
        "system_message = \\\"\\\"\\\"\n",
        "Evaluate this response step-by-step:\n",
        "\n",
        "Step 1: List facts from product data\n",
        "Step 2: Check if response matches those facts\n",
        "Step 3: Identify any hallucinations\n",
        "Step 4: Assess if question is fully answered\n",
        "Step 5: Final judgment (Y/N)\n",
        "\n",
        "Format:\n",
        "Step 1: ...\n",
        "Step 2: ...\n",
        "...\n",
        "Judgment: Y\n",
        "\\\"\\\"\\\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Monitoring and Dashboards\n",
        "\n",
        "### Real-Time Quality Dashboard\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"last_hour\": {\n",
        "        \"responses_evaluated\": 847,\n",
        "        \"pass_rate\": 0.92,\n",
        "        \"avg_latency_ms\": 450,\n",
        "        \"moderation_failures\": 3,\n",
        "        \"accuracy_failures\": 24,\n",
        "        \"completeness_failures\": 45\n",
        "    },\n",
        "    \"top_failure_patterns\": [\n",
        "        \"Missing pricing information\",\n",
        "        \"Mentioned unavailable products\",\n",
        "        \"Incorrect warranty details\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Alerting Thresholds\n",
        "\n",
        "```python\n",
        "if pass_rate < 0.85:\n",
        "    alert_team(\"Quality degradation detected\")\n",
        "    # Auto-switch to fallback system\n",
        "    enable_human_review_mode()\n",
        "\n",
        "if moderation_failures > 10/hour:\n",
        "    alert_security(\"Unusual content policy violations\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Evaluation Best Practices\n",
        "\n",
        "### Implementation\n",
        "1. **Multi-Layer**: Moderation → Factual check → Completeness\n",
        "2. **Binary First**: Y/N for speed, expand to scores later\n",
        "3. **Log Everything**: Track patterns in failures\n",
        "4. **Auto-Fallback**: Have plan B if evaluation fails\n",
        "\n",
        "### When to Use LLM-as-Judge\n",
        "✅ High-volume systems (can't human-review all)  \n",
        "✅ Consistent criteria needed  \n",
        "✅ Cost of bad output > cost of evaluation  \n",
        "✅ Clear right/wrong answers\n",
        "\n",
        "### When NOT to Use\n",
        "❌ Subjective quality (creativity, style)  \n",
        "❌ Low volume (human review feasible)  \n",
        "❌ Cost-sensitive (evaluation adds overhead)\n",
        "\n",
        "### Key Metrics to Track\n",
        "- Pass rate (target: >90%)\n",
        "- Evaluation latency (<500ms)\n",
        "- False positive rate (judge rejects good responses)\n",
        "- False negative rate (judge accepts bad responses)\n",
        "\n",
        "### Next Steps\n",
        "- **G.CustomerServiceBot** - Full system with evaluation\n",
        "- **H.EvaluatingLLMPerformance** - Performance benchmarking\n",
        "- **I.EvaluationUsingRubric** - Multi-dimensional scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57df31e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "another_response = \"life is like a box of chocolates\"\n",
        "q_a_pair = f\"\"\"\n",
        "Customer message: ```{customer_message}```\n",
        "Product information: ```{product_information}```\n",
        "Agent response: ```{another_response}```\n",
        "\n",
        "Does the response use the retrieved information correctly?\n",
        "Does the response sufficiently answer the question?\n",
        "\n",
        "Output Y or N\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {'role': 'system', 'content': system_message},\n",
        "    {'role': 'user', 'content': q_a_pair}\n",
        "]\n",
        "\n",
        "response = get_completion_from_messages(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaeb4463",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

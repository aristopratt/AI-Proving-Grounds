{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a6e2e1e6",
      "metadata": {},
      "source": [
        "# Exploring the ChatGPT Completions API\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook introduces the **OpenAI Chat Completions API**, the foundation for building LLM-powered applications. Understanding this API is critical for:\n",
        "\n",
        "1. **System Design** - Architecture patterns for AI-powered services\n",
        "2. **Cost Optimization** - Token usage directly impacts operational expenses\n",
        "3. **Quality Control** - Temperature and parameter tuning for consistent outputs\n",
        "4. **Production Readiness** - Error handling, rate limits, and monitoring\n",
        "\n",
        "## What We'll Cover\n",
        "\n",
        "1. **Basic Completions** - Single-turn requests and responses\n",
        "2. **Message Roles** - System, user, and assistant message types\n",
        "3. **Temperature Control** - Balancing creativity vs determinism\n",
        "4. **Token Management** - Tracking usage for cost and context limits\n",
        "5. **Helper Functions** - Reusable patterns for production systems\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Roles in Chat API\n",
        "- **system**: Sets behavior, tone, and constraints (invisible to end user)\n",
        "- **user**: Input from the human user\n",
        "- **assistant**: LLM's response (can be used for few-shot examples)\n",
        "\n",
        "### Temperature (0-2)\n",
        "- **0**: Deterministic, consistent (production default)\n",
        "- **1**: Balanced creativity (general use)\n",
        "- **2**: Maximum randomness (creative writing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cf27edd",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Initialize the OpenAI client with API credentials. The `tiktoken` library provides token counting for cost estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7eb4a6fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken  # Token counting library from OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# Load environment variables from .env file (must contain OPENAI_API_KEY)\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "# Set API key for OpenAI client\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280be44b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Simple Completion Helper\n",
        "\n",
        "A basic wrapper for single-turn interactions. This pattern is useful for:\n",
        "- Quick prototyping\n",
        "- Stateless applications\n",
        "- Simple question-answering systems\n",
        "\n",
        "**Default Parameters:**\n",
        "- `temperature=0`: Deterministic output for consistency\n",
        "- `model=\"gpt-3.5-turbo\"`: Cost-effective model for most use cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "502ea223",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize OpenAI client (uses api_key from environment)\n",
        "client = openai.OpenAI()\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Simple wrapper for single-turn completions.\n",
        "    \n",
        "    Args:\n",
        "        prompt: User's input text\n",
        "        model: OpenAI model identifier (default: gpt-3.5-turbo)\n",
        "    \n",
        "    Returns:\n",
        "        str: LLM's response content\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0  # Deterministic output for consistency\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c80bb0",
      "metadata": {},
      "source": [
        "### Test: Basic Factual Query\n",
        "\n",
        "Simple question-answering to verify API connectivity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2e9c0f54",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple factual query - should return \"Paris\"\n",
        "response = get_completion(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4aa77cc",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Prompt Engineering: Character-Level Tasks\n",
        "\n",
        "LLMs struggle with character-level operations (reversing strings, counting letters) due to tokenization. Let's demonstrate the problem and solution.\n",
        "\n",
        "### Issue: String Reversal Without Delimiters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a4610941",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pilpolol\n"
          ]
        }
      ],
      "source": [
        "# Problem: LLM may struggle due to tokenization (sees word chunks, not characters)\n",
        "response = get_completion(\"Take the letters in lollipop \\\n",
        "and reverse them\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8a64e0",
      "metadata": {},
      "source": [
        "**Observation**: LLM may produce incorrect output (e.g., \"popillol\" instead of \"popillol\"). This happens because tokenization treats \"lollipop\" as 1-2 tokens, not individual characters.\n",
        "\n",
        "### Solution: Character Delimiting\n",
        "\n",
        "Explicitly separate characters with hyphens so the LLM processes them individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2219e9c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p-o-p-i-l-l-o-l\n"
          ]
        }
      ],
      "source": [
        "# Solution: Hyphenate characters so LLM processes them individually\n",
        "response = get_completion(\"\"\"Take the letters in \\\n",
        "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
        "print(response)  # Should correctly output: p-o-p-i-l-l-o-l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d16181",
      "metadata": {},
      "source": [
        "**Result**: Correct reversal (p-o-p-i-l-l-o-l).\n",
        "\n",
        "**Key Lesson**: For character-level tasks, provide delimiters to work around tokenization limitations.\n",
        "\n",
        "---\n",
        "\n",
        "## Advanced Completion Helper with Full Control\n",
        "\n",
        "This function exposes all critical parameters for production use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "457cfa28",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages, \n",
        "                                 model=\"gpt-3.5-turbo\", \n",
        "                                 temperature=0, \n",
        "                                 max_tokens=500):\n",
        "    \"\"\"\n",
        "    Production-ready completion helper with full parameter control.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts with 'role' and 'content' keys\n",
        "        model: OpenAI model identifier\n",
        "        temperature: 0-2, controls randomness (0=deterministic, 2=creative)\n",
        "        max_tokens: Maximum tokens in response (limits cost and response length)\n",
        "    \n",
        "    Returns:\n",
        "        str: LLM's response content\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,  # Degree of randomness (0=deterministic, 2=creative)\n",
        "        max_tokens=max_tokens,    # Cap response length and cost\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4038f2ce",
      "metadata": {},
      "source": [
        "## Using System Messages to Constrain Output\n",
        "\n",
        "System messages define behavior, format, and constraints. They're critical for:\n",
        "1. **Output Format** - JSON, length limits, style requirements\n",
        "2. **Tone Control** - Professional, casual, technical\n",
        "3. **Safety Rails** - Content filtering, scope limitations\n",
        "\n",
        "### Example: Length Constraint\n",
        "\n",
        "Force the LLM to respond in exactly one sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5a33486a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once there was a cheerful carrot named Carl who loved dancing in the sunshine.\n"
          ]
        }
      ],
      "source": [
        "# System message: Enforce strict length constraint\n",
        "messages = [  \n",
        "    {'role': 'system',\n",
        "     'content': 'All your responses must be one sentence long.'},  # Hard constraint\n",
        "    {'role': 'user',\n",
        "     'content': 'write me a story about a happy carrot'},  \n",
        "] \n",
        "\n",
        "# temperature=1 allows creativity within the length constraint\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2119f8a",
      "metadata": {},
      "source": [
        "**Observation**: Despite creative temperature, the system message enforces a one-sentence response.\n",
        "\n",
        "**Production Use Cases**:\n",
        "- Chat UI with character limits\n",
        "- SMS/notification systems\n",
        "- Summary generation for dashboards\n",
        "\n",
        "---\n",
        "\n",
        "## Token Counting for Cost & Context Management\n",
        "\n",
        "Production systems must track token usage for:\n",
        "1. **Cost Control** - Billing is per-token (input + output)\n",
        "2. **Context Limits** - Models have maximum context windows (e.g., 4K, 16K, 128K tokens)\n",
        "3. **Performance Monitoring** - Track usage patterns and optimize prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fdd686bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_completion_and_token_count(messages, \n",
        "                                   model=\"gpt-3.5-turbo\", \n",
        "                                   temperature=0, \n",
        "                                   max_tokens=500):\n",
        "    \"\"\"\n",
        "    Completion helper that returns both content and token usage.\n",
        "    Critical for production cost tracking and monitoring.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts\n",
        "        model: OpenAI model identifier\n",
        "        temperature: Randomness control (0-2)\n",
        "        max_tokens: Response length cap\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (response_content, token_dict)\n",
        "            token_dict contains: prompt_tokens, completion_tokens, total_tokens\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, \n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract token usage from response metadata\n",
        "    token_dict = {\n",
        "        'prompt_tokens': response.usage.prompt_tokens,      # Input tokens (your cost)\n",
        "        'completion_tokens': response.usage.completion_tokens,  # Output tokens (your cost)\n",
        "        'total_tokens': response.usage.total_tokens,        # Sum of input + output\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ca0e8e",
      "metadata": {},
      "source": [
        "### Example: Style Control + Token Tracking\n",
        "\n",
        "Using a system message to control output style (Dr. Seuss) while tracking token usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ca4f0948",
      "metadata": {},
      "outputs": [],
      "source": [
        "# System message: Enforce Dr. Seuss style (rhyming, whimsical)\n",
        "messages = [\n",
        "    {'role': 'system', \n",
        "     'content': \"\"\"You are an assistant who responds in the style of Dr Seuss.\"\"\"},\n",
        "    {'role': 'user',\n",
        "     'content': \"\"\"write me a very short poem about a happy carrot\"\"\"},  \n",
        "] \n",
        "\n",
        "# Get both response content and token usage metrics\n",
        "response, token_dict = get_completion_and_token_count(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52db95da",
      "metadata": {},
      "source": [
        "### View Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "510440e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oh, the happy carrot, so bright and so orange,\n",
            "In the garden it grows, a joyful storage.\n",
            "With a leafy green top and a crunchy bite,\n",
            "It brings smiles to all, such a delightful sight!\n"
          ]
        }
      ],
      "source": [
        "# Display the Dr. Seuss-style poem\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d0438dc",
      "metadata": {},
      "source": [
        "**Expected**: Rhyming poem in Dr. Seuss style (e.g., \"In a garden so bright, with sun shining down...\")\n",
        "\n",
        "---\n",
        "\n",
        "### View Token Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "26c8cc3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt_tokens': 35, 'completion_tokens': 44, 'total_tokens': 79}\n"
          ]
        }
      ],
      "source": [
        "# Display token breakdown for cost calculation\n",
        "print(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eff2b13",
      "metadata": {},
      "source": [
        "**Example Output**:\n",
        "```python\n",
        "{\n",
        "    'prompt_tokens': 39,       # System message + user message\n",
        "    'completion_tokens': 52,   # LLM's response\n",
        "    'total_tokens': 91         # Sum (used for billing)\n",
        "}\n",
        "```\n",
        "\n",
        "### Cost Calculation Example (GPT-3.5-Turbo)\n",
        "\n",
        "**Pricing** (as of 2024):\n",
        "- Input: $0.50 / 1M tokens\n",
        "- Output: $1.50 / 1M tokens\n",
        "\n",
        "**This Request**:\n",
        "- Input cost: 39 tokens × $0.50/1M = $0.0000195\n",
        "- Output cost: 52 tokens × $1.50/1M = $0.0000780\n",
        "- **Total: $0.0000975** (~$0.0001)\n",
        "\n",
        "**Production Implications**:\n",
        "- 10K requests/day ≈ $1/day\n",
        "- 1M requests/month ≈ $97.50/month\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Production Patterns\n",
        "\n",
        "### Helper Function Selection\n",
        "\n",
        "| Use Case | Function | Key Feature |\n",
        "|----------|----------|-------------|\n",
        "| **Quick Prototyping** | `get_completion()` | Simple, single-turn |\n",
        "| **Multi-turn Chat** | `get_completion_from_messages()` | Full message history support |\n",
        "| **Cost Tracking** | `get_completion_and_token_count()` | Returns usage metrics |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **System Messages**\n",
        "   - Define behavior, format, tone\n",
        "   - Use for output constraints (length, structure)\n",
        "   - Invisible to end users\n",
        "\n",
        "2. **Temperature Control**\n",
        "   - 0: Deterministic (production default for consistency)\n",
        "   - 0.5-1: Balanced (general use)\n",
        "   - 1.5-2: Creative (writing, brainstorming)\n",
        "\n",
        "3. **Token Management**\n",
        "   - Always track usage in production\n",
        "   - Set `max_tokens` to prevent runaway costs\n",
        "   - Monitor prompt efficiency (shorter = cheaper)\n",
        "\n",
        "4. **Error Handling** (not shown, but critical)\n",
        "   - Wrap API calls in try/except\n",
        "   - Handle rate limits (exponential backoff)\n",
        "   - Log failures for debugging\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **B.EvaluateAndClassifyInputs** - Input validation and routing\n",
        "- **C.InputModerationTechniques** - Content safety and filtering\n",
        "- **D.ChainOfThoughtReasoning** - Complex reasoning patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4937a73e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
